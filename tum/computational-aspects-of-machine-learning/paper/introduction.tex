\section{Introduction}
Even though on-line learning algorithms date back to 1940 when they were first developed by Professor Bernard Widrow and Ted Hoff, his graduate student, under the name ADALINE \cite{adaline}, which essentially comprised stochastic gradient descent, their extensive usage and popularity was not as much advocated. With increasing size of analyzed data, more and more attention was drawn to on-line learning, where computational complexity of an algorithm becomes the limiting factor. In this paper we will present an overview of various gradient descent methods and reason in favor of SGD within the large-scale problem setting and in context of asymptotic analysis.

Furthermore, in the last decade phenomena known as Big Data and Internet of Things have drawn even more attention to rapidly increasing rates of collected data. These phenomena and a result of industry scale data sets growing from Gigabytes to Petabytes of data storage are putting increasing pressure on data processing, where sequential algorithms are no longer sufficient. Under such restrictions we will present an overview of possible parallelization approaches of SGD and guide the reader through pressing problems of parallelization and their potential remedies. We will also take a closer look at work of practitioners within the industry and present some of the latest findings in parallelizing SGD to ensure its scalability to numerous processing units.