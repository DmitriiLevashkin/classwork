\section{Conclusion and further work}
Within this paper we have presented and justified the rationale behind using on-line learning algorithms for large-scale problems, we have pointed out advantages and disadvantages of various parallelization approaches and provided some experimental results to supplement our arguments. Even though the reader was guided to conclude that a specific parallelization idea dominates the realm, all of the presented schemes have their drawbacks. Supplying improvement of the latter always comes with a cost and any of the parallelization methods might turn to be an optimal choice for a specific problem setting. 

However, in the ultimate scenario where amount of data is assumed to be infinite, as in processing of real-time streaming data, we conclude that the latter approach, namely parallelized asynchronous stochastic gradient descent with adaptive learning rates, should show the best results. This topic presents itself as an intimidating research prospect and is encouraged to be studied separately.