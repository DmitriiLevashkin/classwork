\documentclass{article}
\usepackage{ws_template}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{booktabs}

% please submit the corresponding pdf by email to
% homework@class,brml.org, and write "homework sheet xx" in the 
% title.  No more, no less!  (Instead of xx, however,
% put the decimal number of the homework sheet.)

% Please update the following line, only change XX to the homework
% sheet number
\title{homework sheet 04}


\author{
\name{Denys Sobchyshak}\\
\imat{03636581}\\
\email{denys.sobchyshak@gmail.com}
\And
\name{Sergey Zakharov} \\
\imat{03636642}\\
\email{ga39pad@mytum.de}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.



\begin{document}
\maketitle

\section{Basic Probability}
\textbf{Problem 1:} \\
Let $ X_1 \sim N(\mu_1, \Sigma_1)$ and $ X_2 \sim N(\mu_2, \Sigma_2)$. We can form a Gaussian random vector R, which consists of the vectors $X_1$ and $X_2$:
\[ R = \binom{X_1}{X_2} \sim N \left(\binom{\mu_1}{\mu_2}, \begin{pmatrix} \Sigma_1 & 0\\ 0 & \Sigma_2  \end{pmatrix} \right)\]
The sum of $X_1$ and $X_2$ can be written as $S = IR$, where $I = [I I]$. That means that the sum is also a Gaussian which has a mean $\mu_1 + \mu_2$ and covariance $\Sigma_1 + \Sigma_2$.\\

\textbf{Problem 2:} \\
Suppose we have a fair coin. There are 3 random variables: $X_1$ and $X_2$ correspond to the 2 independent tosses of the coin; $X_3$ equals 1 if two tosses resulted in heads. The probability $P(X_i) = \frac{1}{2}$ for each $X_i$, where $i \in (1,2,3)$
All the variables are pairwise independent since: $P(X_i \cap X_j) = P(X_i)P(X_j) = \frac{1}{4}$, where $ i \neq j$.
But they are not mutually independent since: $P(X_1 \cap X_2 \cap X_3) =\frac{1}{4} \neq P(X_1)P(X_2)P(X_3) = \frac{1}{8}$\\

\textbf{Problem 3:} \\
For discrete case:
\begin{flalign*}
	&Var(X+Y)&\\
	&=\sum_{x}\sum_{y} (x+y)^2 P_{XY}(x,y)-(E(X+Y))^2 &\\
	&=\sum_{x}\sum_{y} x^2 P_{XY}(x,y)+\sum_{x}\sum_{y} 2xy2 P_{XY}(x,y)+\sum_{y}\sum_{x}y^2 P_{XY}(x,y)-
		(E(X))^2-2E(X)E(Y)-(E(Y))^2 &\\
	&=\sum_{x} x^2 P_{X}(x)-(E(X))^2+\sum_{y}y^2 P_{Y}(y)-(E(Y))^2+\sum_{x}\sum_{y} 2xy2 P_{XY}(x,y)-2E(X)E(Y) &\\
	&=E(X^2)-(E(X))^2+E(Y^2)-(E(Y))^2+2(E(XY)âˆ’E(X)E(Y)) &\\
	&=Var(X)+Var(Y)+2Cov(X,Y)
\end{flalign*}\\

\textbf{Problem 4:} \\
Correlation is defined as $\rho(X,Y)= \sum_{i=1}^{n} \frac{1}{n}\frac{x_i-\mu_x}{\sigma_x}\frac{y_i-\mu_y}{\sigma_y} =\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$. To prove that it lies in the range $[-1, 1]$ we need to first find upper bound of it's absolute value. Applying Cauchy-Schwarz inequality we get that:\\
\[ |Cov(X,Y)|^2 \leq  Var(X)Var(Y) \]
\[ |Cov(X,Y)| \leq  \sqrt{Var(X)Var(Y)} \]
Using the result above in correlation formula we obtain:\\
\[ |\rho|=|\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}| \leq |\frac{\sqrt{Var(X)Var(Y)}}{\sqrt{Var(X)Var(Y)}}|=1 \]
One can notice that depending on divergence of the pairwise discrete values of random variables their product will result either in negative or positive value, which means that the variables either diverge from mean in the same maner (are larger or lower then mean together) or in the opposite one (one is larger while the other is smaller) which intuitevly describes correlation and also gives it either negative or positive value.\\

\textbf{Problem 5:} \\
From properties of distribution moments we can deduce:
\[ Var(Y)=Var(aX+b)=a^2Var(X) \]
\[ Cov(X,Y)=Cov(X,aX+b)=aCov(X,X)=a(Var(X))^2 \]
\[ \rho(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}=\frac{a(Var(X))^2}{\sqrt{Var(X)a^2Var(X)}} \]
Now depending on the sign of $a$ our corellation will be equal to 1 or -1.\\

\textbf{Problem 6:} \\
\[p(X,Y) = \frac{cov[X,Y]}{\sqrt{var[X]var[Y]}} = \frac{E[XY] - E[X]E[Y]}{\sqrt{var[X]var[Y]}} \overset{!}{=} 0 \Leftrightarrow E[XY] = E[X]E[Y] \]
Y is uniformly distributed on [0,1] and we know that if $V \sim U(a,b)$ $E[V] = (a + b)/2$.\\
So $E[X] = (-1 + 1)/2 = 0$ and $E[Y] = (0 + 1)/2 = 1/2$.
Finally $E[XY] = \int^1_{-1}{x^3 dx} = 0$.\\
Therefore $E[XY] = E[X]E[Y] = 0 \Leftrightarrow p(X,Y) = 0$\\

\textbf{Problem 7:} \\
$ Cov[X,Y] = Cov[Y,X] = 0$ because $\rho(X,Y) = 0$. So, $\Sigma_z = \begin{pmatrix} \sigma^2_X & 0\\ 0 & \sigma^2_Y \end{pmatrix}$ and $\mu_Z = \binom{\mu_X}{\mu_Y}$ Then:
\begin{equation*}
	\begin{aligned}
p_Z(z) & = \frac{1}{2\pi \Sigma_Z} exp \left(\frac{-1}{2} (z-\mu_z)^T \Sigma^{-1}_Z(z-\mu_z)\right)\\ 
& = \frac{1}{2 \pi \sigma^2_X 2 \pi \sigma^2_Y} exp \left( \frac{-(x-\mu_X)^2}{2 \sigma^2_X} + \frac{-(y-\mu_Y)^2}{2 \sigma^2_Y} \right) \\
& = p_X(x)p_Y(y)
	\end{aligned}
\end{equation*}

\textbf{Problem 8:} \\
\[ H[X] = -\sum_x{ln \frac{1}{1/p(x)}p(x)} \leq -\ln \frac{1}{\sum_x{1/p(x)p(x)}} = \ln n\]
\[ D_{KL}(p||q) = \sum_x{p(x) \ln \frac{p(x)}{q(x)}} = -\sum_x{p(x) \ln \frac{q(x)}{p(x)}} \geq -\ln\sum_x{p(x) \frac{q(x)}{p(x)}}= -\ln \sum_xq(x) = -\ln(1) =0\]


\end{document}
