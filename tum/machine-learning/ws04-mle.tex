\documentclass{article}
\usepackage{ws_template}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{booktabs}

\title{homework sheet 04}

\author{
	\name{Denys Sobchyshak}\\
	\imat{03636581}\\
	\email{denys.sobchyshak@gmail.com}
	\And
	\name{Sergey Zakharov}\\
	\imat{03636642}\\
	\email{ga39pad@mytum.de}
}


\begin{document}
\maketitle

\section{Still refreshing}
\textbf{Problem 1:} \\\\
	 Let X represent the number of flips which were made. Then the possible values of X
	are $1, 2, ... ,$ and the distribution function of X is deÔ¨Åned by.
	\[ m(i) =\frac{1}{2^i} \]
	Thus,
	\begin{eqnarray*}
  	E(X) & = & \sum_{i=1}^{\infty}{i \frac{1}{2^i}} \\
 	  & = & 1+ \frac{1}{2} +\frac{1}{2^2} + \dots \\
  	 & = & 2
	\end{eqnarray*}

	So, the expected number of flips until one head is thrown is 2. \\
	Now, Let T represent the expexted number of tails. The possible values of T are 0, 1 and 2. The corresponding probabilities are 1/4, 1/2 and 1/4. Thus,
	\[ E(T) = 0(\frac{1}{4}) + 1(\frac{1}{2}) + 2 (\frac{1}{4}) = 1\]
	The expected number of heads equals to the expected number of tails $E(T) = E(H) = 1$

\section{Parameter Estimation}
\subsection{Coins}
\textbf{Problem 2:}\\\\
	The likelihood function $L(\theta)$ is, by definition:
	\begin{eqnarray*}
  	L(\theta) & = &\prod_{i=i}^{n}{P(X_i = x_i|\theta)} \\
 	  & = & \theta^{x_1}(1-\theta)^{1-x_1} \times \theta^{x_2}(1-\theta)^{1-x_2} \times \dots \times \theta^{x_n}(1-\theta)^{1-x_n} \\
  	 & = & \theta^{\sum{x_i}}(1-\theta)^{n-\sum{x_i}}
	\end{eqnarray*}
Now, in order to implement the method of maximum likelihood, we need to find the $\theta$ that maximizes the likelihood $L(\theta)$.
\[ ln L(\theta) = (\sum{x_i})ln(\theta) + (n-\sum{x_i})ln(1-\theta) \]
Differentiating with respect to $\theta$ results in:
\[ \frac{d ln L(\theta)}{d\theta} = \frac{\sum{x_i}}{\theta} - \frac{n-\sum{x_i}}{1-\theta} \]
Setting the derivative to zero we get:
\begin{eqnarray*}
(\sum{x_i})(1- \theta) - (n-\sum{x_i})\theta = 0 \\
\sum{x_i} - n\theta = 0
\end{eqnarray*}
Therefore the MLE for $\theta$ is:
\[ \hat{\theta} = \frac{\sum{x_i}}{n} \]

\textbf{Problem 3:}\\\\
Given the observations and $\alpha$,$\beta$ as beta distribution parameters we can write the expected posterior mean for $\mu$ as follows:
\[ E_{\mu}[\mu|\mathcal{D}]=\frac{m+\alpha}{m+\ell+\alpha+\beta}=\frac{m}{m+\ell+\alpha+\beta}+
\frac{\alpha}{m+\ell+\alpha+\beta}\]
However:
\[\frac{m}{m+\ell+\alpha+\beta}=\frac{\frac{m}{m+\ell}}{\frac{m+\ell+\alpha+\beta}{m+\ell}}\]
\[\frac{\alpha}{m+\ell+\alpha+\beta}=\frac{\frac{\alpha}{\alpha+\beta}}{\frac{m+\ell+\alpha+\beta}{\alpha+\beta}}\]
Since $\frac{\alpha}{\alpha+\beta}$ is the prior mean value of $\mu$ and $\frac{m}{m+\ell}$ is the maximum likelyhood estimate we have shown what was being asked.

\subsection{Poisson distribution}
\textbf{Problem 4:}\\\\
The likelihood function is:
\[ L(\lambda) =\prod_{i=1}^{n}{\frac{\lambda^{x_i}e^{-\lambda}}{x_i !}} = \frac{\lambda^{\sum {x_i}}e^{-\lambda}}{\prod{x_i !}} \]
Then, taking the natural logarithm, we have:
\[ ln L(\lambda) = \sum_{i=1}^{n}{x_i ln \lambda} - n\lambda - \sum_{i=1}^{n}{ln(x_i !)} \]
Differentiating with respect to $\lambda$ results in:
\[ \frac{d ln L(\lambda)}{d\lambda} = \frac{\sum{x_i}}{\lambda} - n \]
Setting the derivative to zero we get:
\[ \frac{\sum{x_i}}{\lambda} - n = 0 \]
Therefore the MLE for $\lambda$ is:
\[  \hat{\lambda} = \frac{\sum{x_i}}{n}\]
To show that the maximum likelihood estimator is an unbiased estimator of $\lambda$ we have to show that $E(\hat{\lambda}) = p$.
\[ E(\hat{\lambda}) = E() = \frac{1}{n}\sum_{i=1}^{n}{E(X_i)} =\frac{1}{n}\sum_{i=1}^{n}{p} = \frac{1}{n}(np) = p \]
Thus, the estimator is unbiased.	
	
\end{document}

